---
comments: true
description: å­¦ä¹ å¦‚ä½•åœ¨å•ä¸ªæˆ–å¤šä¸ªGPUä¸Šä½¿ç”¨YOLOv5è¿›è¡Œæ•°æ®é›†è®­ç»ƒã€‚åŒ…æ‹¬è®¾ç½®ã€è®­ç»ƒæ¨¡å¼å’Œç»“æœåˆ†æï¼Œä»¥é«˜æ•ˆåˆ©ç”¨å¤šä¸ªGPUã€‚
keywords: YOLOv5, å¤šGPUè®­ç»ƒ, YOLOv5è®­ç»ƒ, æ·±åº¦å­¦ä¹ , æœºå™¨å­¦ä¹ , ç›®æ ‡æ£€æµ‹, Ultralytics
---

ğŸ“š æœ¬æŒ‡å—è§£é‡Šäº†å¦‚ä½•æ­£ç¡®ä½¿ç”¨**å¤šä¸ª**GPUæ¥ä½¿ç”¨YOLOv5 ğŸš€åœ¨å•å°æˆ–å¤šå°æœºå™¨ä¸Šè®­ç»ƒæ•°æ®é›†ã€‚

## å¼€å§‹ä¹‹å‰

å…‹éš†ä»“åº“å¹¶åœ¨[**Python>=3.8.0**](https://www.python.org/)ç¯å¢ƒä¸­å®‰è£…[requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt)ï¼ŒåŒ…æ‹¬[**PyTorch>=1.8**](https://pytorch.org/get-started/locally/)ã€‚[æ¨¡å‹](https://github.com/ultralytics/yolov5/tree/master/models)å’Œ[æ•°æ®é›†](https://github.com/ultralytics/yolov5/tree/master/data)å°†è‡ªåŠ¨ä»æœ€æ–°çš„YOLOv5[å‘å¸ƒ](https://github.com/ultralytics/yolov5/releases)ä¸­ä¸‹è½½ã€‚

```bash
git clone https://github.com/ultralytics/yolov5  # å…‹éš†ä»“åº“
cd yolov5
pip install -r requirements.txt  # å®‰è£…ä¾èµ–

```

ğŸ’¡ ProTip! **Docker Image** is recommended for all Multi-GPU trainings. See [Docker Quickstart Guide](../environments/docker_image_quickstart_tutorial.md) <a href="https://hub.docker.com/r/ultralytics/yolov5"><img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></a>
ğŸ’¡ ä¸“å®¶æç¤ºï¼Dockeré•œåƒæ¨èç”¨äºæ‰€æœ‰å¤šGPUè®­ç»ƒã€‚å‚è§Dockerå¿«é€Ÿå…¥é—¨æŒ‡å— <a href="https://hub.docker.com/r/ultralytics/yolov5"><img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></a>


ğŸ’¡ ProTip! `torch.distributed.run` replaces `torch.distributed.launch` in **PyTorch>=1.9**. See [docs](https://pytorch.org/docs/stable/distributed.html) for details.
ğŸ’¡ ä¸“å®¶æç¤ºï¼åœ¨PyTorch>=1.9ä¸­ï¼Œtorch.distributed.runæ›¿ä»£äº†torch.distributed.launchã€‚è¯¦è§æ–‡æ¡£ã€‚


## Training

Select a pretrained model to start training from. Here we select [YOLOv5s](https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml), the smallest and fastest model available. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models. We will train this model with Multi-GPU on the [COCO](https://github.com/ultralytics/yolov5/blob/master/data/scripts/get_coco.sh) dataset.
è®­ç»ƒ
é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¼€å§‹è®­ç»ƒã€‚è¿™é‡Œæˆ‘ä»¬é€‰æ‹©YOLOv5sï¼Œè¿™æ˜¯å¯ç”¨çš„æœ€å°å’Œæœ€å¿«çš„æ¨¡å‹ã€‚æŸ¥çœ‹æˆ‘ä»¬çš„READMEè¡¨æ ¼ï¼Œäº†è§£æ‰€æœ‰æ¨¡å‹çš„å®Œæ•´æ¯”è¾ƒã€‚æˆ‘ä»¬å°†åœ¨COCOæ•°æ®é›†ä¸Šä½¿ç”¨å¤šGPUè®­ç»ƒè¿™ä¸ªæ¨¡å‹ã€‚
<p align="center"><img width="700" alt="YOLOv5 Models" src="https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png"></p>

### Single GPU å•GPU

```bash
python train.py  --batch 64 --data coco.yaml --weights yolov5s.pt --device 0
```

### Multi-GPU [DataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel) Mode (âš ï¸ not recommended)
å¤šGPU DataParallelæ¨¡å¼ (âš ï¸ ä¸æ¨è)
ä½ å¯ä»¥å¢åŠ deviceæ¥ä½¿ç”¨å¤šGPUçš„DataParallelæ¨¡å¼ã€‚
You can increase the `device` to use Multiple GPUs in DataParallel mode.

```bash
python train.py  --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1
```

This method is slow and barely speeds up training compared to using just 1 GPU.
è¿™ç§æ–¹æ³•é€Ÿåº¦å¾ˆæ…¢ï¼Œå‡ ä¹æ²¡æœ‰åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œç›¸æ¯”äºä»…ä½¿ç”¨1ä¸ªGPUã€‚

### Multi-GPU [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel) Mode (âœ… recommended)
å¤šGPU DistributedDataParallelæ¨¡å¼ (âœ… æ¨è)
ä½ éœ€è¦ä¼ é€’python -m torch.distributed.run --nproc_per_nodeï¼Œåè·Ÿé€šå¸¸çš„å‚æ•°ã€‚
You will have to pass `python -m torch.distributed.run --nproc_per_node`, followed by the usual arguments.

```bash
python -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1
```

`--nproc_per_node` specifies how many GPUs you would like to use. In the example above, it is 2.
`--batch ` is the total batch-size. It will be divided evenly to each GPU. In the example above, it is 64/2=32 per GPU.
--nproc_per_nodeæŒ‡å®šä½ æƒ³ä½¿ç”¨å¤šå°‘ä¸ªGPUã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæ˜¯2ä¸ªã€‚
--batchæ˜¯æ€»æ‰¹é‡å¤§å°ã€‚å®ƒå°†å¹³å‡åˆ†é…ç»™æ¯ä¸ªGPUã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæ˜¯64/2=æ¯ä¸ªGPU 32ã€‚

ä¸Šè¿°ä»£ç å°†ä½¿ç”¨GPU 0... (N-1)ã€‚
The code above will use GPUs `0... (N-1)`.

<details>
  <summary>Use specific GPUs (click to expand)</summary>
  <summary>ä½¿ç”¨ç‰¹å®šçš„GPUï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

You can do so by simply passing `--device` followed by your specific GPUs. For example, in the code below, we will use GPUs `2,3`.
ä½ å¯ä»¥é€šè¿‡ç®€å•åœ°ä¼ é€’--deviceåè·Ÿä½ çš„ç‰¹å®šGPUæ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨GPU 2,3ã€‚


```bash
python -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --device 2,3
```

</details>

<details>
  <summary>Use SyncBatchNorm (click to expand)</summary>
  <summary>ä½¿ç”¨SyncBatchNormï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

[SyncBatchNorm](https://pytorch.org/docs/master/generated/torch.nn.SyncBatchNorm.html) could increase accuracy for multiple gpu training, however, it will slow down training by a significant factor. It is **only** available for Multiple GPU DistributedDataParallel training.
SyncBatchNormå¯ä»¥æé«˜å¤šGPUè®­ç»ƒçš„å‡†ç¡®æ€§ï¼Œä½†æ˜¯ï¼Œå®ƒä¼šæ˜¾è‘—å‡æ…¢è®­ç»ƒé€Ÿåº¦ã€‚å®ƒä»…é€‚ç”¨äºå¤šGPU DistributedDataParallelè®­ç»ƒã€‚

It is best used when the batch-size on **each** GPU is small (<= 8).
å½“æ¯ä¸ªGPUä¸Šçš„æ‰¹é‡å¤§å°å¾ˆå°ï¼ˆ<= 8ï¼‰æ—¶ï¼Œæœ€å¥½ä½¿ç”¨å®ƒã€‚

To use SyncBatchNorm, simple pass `--sync-bn` to the command like below,
è¦ä½¿ç”¨SyncBatchNormï¼Œåªéœ€å°†--sync-bnä¼ é€’ç»™å‘½ä»¤ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ

```bash
python -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --sync-bn
```

</details>

<details>
  <summary>Use Multiple machines (click to expand)</summary>
  <summary>ä½¿ç”¨å¤šå°æœºå™¨ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

This is **only** available for Multiple GPU DistributedDataParallel training.
è¿™ä»…é€‚ç”¨äºå¤šGPU DistributedDataParallelè®­ç»ƒã€‚

Before we continue, make sure the files on all machines are the same, dataset, codebase, etc. Afterward, make sure the machines can communicate to each other.
åœ¨ç»§ç»­ä¹‹å‰ï¼Œç¡®ä¿æ‰€æœ‰æœºå™¨ä¸Šçš„æ–‡ä»¶ç›¸åŒï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€ä»£ç åº“ç­‰ã€‚ä¹‹åï¼Œç¡®ä¿æœºå™¨ä¹‹é—´å¯ä»¥ç›¸äº’é€šä¿¡ã€‚

You will have to choose a master machine(the machine that the others will talk to). Note down its address(`master_addr`) and choose a port(`master_port`). I will use `master_addr = 192.168.1.1` and `master_port = 1234` for the example below.
ä½ éœ€è¦é€‰æ‹©ä¸€å°ä¸»æœºï¼ˆå…¶ä»–æœºå™¨å°†ä¸ä¹‹é€šä¿¡ï¼‰ã€‚è®°ä¸‹å…¶åœ°å€ï¼ˆmaster_addrï¼‰å¹¶é€‰æ‹©ä¸€ä¸ªç«¯å£ï¼ˆmaster_portï¼‰ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨master_addr = 192.168.1.1å’Œmaster_port = 1234ã€‚


To use it, you can do as the following,
è¦ä½¿ç”¨å®ƒï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼Œ


```bash
# On master machine 0
python -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank 0 --master_addr "192.168.1.1" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights ''
```

```bash
# On machine R
python -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank R --master_addr "192.168.1.1" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights ''
```

where `G` is number of GPU per machine, `N` is the number of machines, and `R` is the machine number from `0...(N-1)`. Let's say I have two machines with two GPUs each, it would be `G = 2` , `N = 2`, and `R = 1` for the above.
å…¶ä¸­Gæ˜¯æ¯å°æœºå™¨çš„GPUæ•°é‡ï¼ŒNæ˜¯æœºå™¨æ•°é‡ï¼ŒRæ˜¯æœºå™¨ç¼–å·ä»0...(N-1)ã€‚å‡è®¾æˆ‘æœ‰ä¸¤å°æœºå™¨ï¼Œæ¯å°æœ‰ä¸¤ä¸ªGPUï¼Œä¸Šè¿°ä¾‹å­ä¸­G = 2 ï¼ŒN = 2ï¼ŒR = 1ã€‚


Training will not start until <b>all </b> `N` machines are connected. Output will only be shown on master machine!
è®­ç»ƒä¸ä¼šå¼€å§‹ç›´åˆ°<b>æ‰€æœ‰</b>Nå°æœºå™¨è¿æ¥ã€‚è¾“å‡ºå°†ä»…åœ¨ä¸»æœºä¸Šæ˜¾ç¤ºï¼


</details>

### Notes

- Windows support is untested, Linux is recommended.
- `--batch ` must be a multiple of the number of GPUs.
- GPU 0 will take slightly more memory than the other GPUs as it maintains EMA and is responsible for checkpointing etc.
- If you get `RuntimeError: Address already in use`, it could be because you are running multiple trainings at a time. To fix this, simply use a different port number by adding `--master_port` like below,
æ³¨æ„äº‹é¡¹
Windowsæ”¯æŒæœªç»æµ‹è¯•ï¼Œæ¨èä½¿ç”¨Linuxã€‚
--batchå¿…é¡»æ˜¯GPUæ•°é‡çš„å€æ•°ã€‚
GPU 0å°†å ç”¨æ¯”å…¶ä»–GPUç¨å¤šçš„å†…å­˜ï¼Œå› ä¸ºå®ƒç»´æŠ¤EMAå¹¶è´Ÿè´£æ£€æŸ¥ç‚¹ç­‰ã€‚
å¦‚æœä½ æ”¶åˆ°RuntimeError: Address already in useï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ åŒæ—¶è¿è¡Œäº†å¤šä¸ªè®­ç»ƒã€‚è¦è§£å†³æ­¤é—®é¢˜ï¼Œåªéœ€ä½¿ç”¨ä¸åŒçš„ç«¯å£å·ï¼Œæ·»åŠ --master_portï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ
```bash
python -m torch.distributed.run --master_port 1234 --nproc_per_node 2 ...
```

## Results
ç»“æœ
åœ¨æ‹¥æœ‰8x A100 SXM4-40GBçš„AWS EC2 P4då®ä¾‹ä¸Šå¯¹YOLOv5lè¿›è¡Œ1ä¸ªCOCO epochçš„DDPæ€§èƒ½åˆ†æç»“æœã€‚
DDP profiling results on an [AWS EC2 P4d instance](../environments/aws_quickstart_tutorial.md) with 8x A100 SXM4-40GB for YOLOv5l for 1 COCO epoch.

<details>
  <summary>Profiling code</summary>
  <summary>æ€§èƒ½åˆ†æä»£ç </summary>

```bash
# å‡†å¤‡
t=ultralytics/yolov5:latest && sudo docker pull $t && sudo docker run -it --ipc=host --gpus all -v "$(pwd)"/coco:/usr/src/coco $t
pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html
cd .. && rm -rf app && git clone https://github.com/ultralytics/yolov5 -b master app && cd app
cp data/coco.yaml data/coco_profile.yaml

# æ€§èƒ½åˆ†æ
python train.py --batch-size 16 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0
python -m torch.distributed.run --nproc_per_node 2 train.py --batch-size 32 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1
python -m torch.distributed.run --nproc_per_node 4 train.py --batch-size 64 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1,2,3
python -m torch.distributed.run --nproc_per_node 8 train.py --batch-size 128 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1,2,3,4,5,6,7
```

</details>

| GPUs<br>A100 | batch-size | CUDA_mem<br><sup>device0 (G) | COCO<br><sup>train | COCO<br><sup>val |
|--------------|------------|------------------------------|--------------------|------------------|
| 1x           | 16         | 26GB                         | 20:39              | 0:55             |
| 2x           | 32         | 26GB                         | 11:43              | 0:57             |
| 4x           | 64         | 26GB                         | 5:57               | 0:55             |
| 8x           | 128        | 26GB                         | 3:09               | 0:57             |

## FAQ

If an error occurs, please read the checklist below first! (It could save your time)
å¦‚æœå‡ºç°é”™è¯¯ï¼Œè¯·é¦–å…ˆé˜…è¯»ä¸‹é¢çš„æ¸…å•ï¼ï¼ˆå®ƒå¯ä»¥èŠ‚çœä½ çš„æ—¶é—´ï¼‰


<details>
  <summary>Checklist (click to expand) </summary>

<ul>
    <li>Have you properly read this post?  </li>
    <li>Have you tried to re-clone the codebase? The code changes <b>daily</b>.</li>
    <li>Have you tried to search for your error? Someone may have already encountered it in this repo or in another and have the solution. </li>
    <li>Have you installed all the requirements listed on top (including the correct Python and Pytorch versions)? </li>
    <li>Have you tried in other environments listed in the "Environments" section below? </li>
    <li>Have you tried with another dataset like coco128 or coco2017? It will make it easier to find the root cause. </li>
</ul>
<ul>
    <li>ä½ æ˜¯å¦æ­£ç¡®é˜…è¯»äº†è¿™ç¯‡æ–‡ç« ï¼Ÿ</li>
    <li>ä½ æ˜¯å¦å°è¯•é‡æ–°å…‹éš†ä»£ç åº“ï¼Ÿä»£ç <b>æ¯æ—¥</b>éƒ½ä¼šæ›´æ”¹ã€‚</li>
    <li>ä½ æ˜¯å¦å°è¯•æœç´¢ä½ çš„é”™è¯¯ï¼Ÿå¯èƒ½æœ‰äººå·²ç»åœ¨è¿™ä¸ªä»“åº“æˆ–å…¶ä»–åœ°æ–¹é‡åˆ°äº†è¿™ä¸ªé—®é¢˜å¹¶æœ‰äº†è§£å†³æ–¹æ¡ˆã€‚</li>
    <li>ä½ æ˜¯å¦å®‰è£…äº†é¡¶éƒ¨åˆ—å‡ºçš„æ‰€æœ‰ä¾èµ–é¡¹ï¼ˆåŒ…æ‹¬æ­£ç¡®çš„Pythonå’ŒPytorchç‰ˆæœ¬ï¼‰ï¼Ÿ</li>
    <li>ä½ æ˜¯å¦åœ¨"ç¯å¢ƒ"éƒ¨åˆ†åˆ—å‡ºçš„å…¶ä»–ç¯å¢ƒä¸­å°è¯•è¿‡ï¼Ÿ</li>
    <li>ä½ æ˜¯å¦å°è¯•ä½¿ç”¨å…¶ä»–æ•°æ®é›†ï¼Œå¦‚coco128æˆ–coco2017ï¼Ÿè¿™æ ·ä¼šæ›´å®¹æ˜“æ‰¾åˆ°æ ¹æœ¬åŸå› ã€‚</li>
</ul>
å¦‚æœä½ å·²ç»å®Œæˆäº†ä¸Šè¿°æ‰€æœ‰æ­¥éª¤ï¼Œè¯·æŒ‰ç…§æ¨¡æ¿æä¾›å°½å¯èƒ½å¤šçš„è¯¦ç»†ä¿¡æ¯æäº¤ä¸€ä¸ªIssueã€‚
If you went through all the above, feel free to raise an Issue by giving as much detail as possible following the template.

</details>

## Supported Environments
æ”¯æŒçš„ç¯å¢ƒ
Ultralyticsæä¾›äº†ä¸€ç³»åˆ—å³ç”¨å‹ç¯å¢ƒï¼Œæ¯ä¸ªç¯å¢ƒéƒ½é¢„è£…äº†åŸºæœ¬ä¾èµ–é¡¹ï¼Œå¦‚CUDAã€CUDNNã€Pythonå’ŒPyTorchï¼Œä»¥å¯åŠ¨ä½ çš„é¡¹ç›®ã€‚

å…è´¹GPUç¬”è®°æœ¬: <a href="https://bit.ly/yolov5-paperspace-notebook"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"></a> <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> <a href="https://www.kaggle.com/ultralytics/yolov5"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>
è°·æ­Œäº‘: GCPå¿«é€Ÿå…¥é—¨æŒ‡å—
äºšé©¬é€Š: AWSå¿«é€Ÿå…¥é—¨æŒ‡å—
Azure: AzureMLå¿«é€Ÿå…¥é—¨æŒ‡å—
Docker: Dockerå¿«é€Ÿå…¥é—¨æŒ‡å— <a href="https://hub.docker.com/r/ultralytics/yolov5"><img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></a>
Ultralytics provides a range of ready-to-use environments, each pre-installed with essential dependencies such as [CUDA](https://developer.nvidia.com/cuda), [CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/), to kickstart your projects.

- **Free GPU Notebooks**: <a href="https://bit.ly/yolov5-paperspace-notebook"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"></a> <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> <a href="https://www.kaggle.com/ultralytics/yolov5"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>
- **Google Cloud**: [GCP Quickstart Guide](../environments/google_cloud_quickstart_tutorial.md)
- **Amazon**: [AWS Quickstart Guide](../environments/aws_quickstart_tutorial.md)
- **Azure**: [AzureML Quickstart Guide](../environments/azureml_quickstart_tutorial.md)
- **Docker**: [Docker Quickstart Guide](../environments/docker_image_quickstart_tutorial.md) <a href="https://hub.docker.com/r/ultralytics/yolov5"><img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></a>

## Project Status

<a href="https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml"><img src="https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg" alt="YOLOv5 CI"></a>

This badge indicates that all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are successfully passing. These CI tests rigorously check the functionality and performance of YOLOv5 across various key aspects: [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py). They ensure consistent and reliable operation on macOS, Windows, and Ubuntu, with tests conducted every 24 hours and upon each new commit.

## Credits

We would like to thank @MagicFrogSJTU, who did all the heavy lifting, and @glenn-jocher for guiding us along the way.
