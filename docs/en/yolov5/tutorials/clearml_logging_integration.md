---
comments: true
description: äº†è§£å¦‚ä½•é€šè¿‡ClearMLå¢å¼ºæ‚¨çš„YOLOv5ç®¡é“â€”â€”è·Ÿè¸ªè®­ç»ƒè¿è¡Œã€ç‰ˆæœ¬æ§åˆ¶æ•°æ®ã€è¿œç¨‹ç›‘æ§æ¨¡å‹å’Œä¼˜åŒ–æ€§èƒ½ã€‚
keywords: ClearML, YOLOv5, Ultralytics, AIå·¥å…·ç®±, è®­ç»ƒæ•°æ®, è¿œç¨‹è®­ç»ƒ, è¶…å‚æ•°ä¼˜åŒ–, YOLOv5æ¨¡å‹
---

# ClearML é›†æˆ

<img align="center" src="https://github.com/thepycoder/clearml_screenshots/raw/main/logos_dark.png#gh-light-mode-only" alt="Clear|ML"><img align="center" src="https://github.com/thepycoder/clearml_screenshots/raw/main/logos_light.png#gh-dark-mode-only" alt="Clear|ML">

## å…³äº ClearML

[ClearML](https://clear.ml/) æ˜¯ä¸€ä¸ª[å¼€æº](https://github.com/allegroai/clearml)å·¥å…·ç®±ï¼Œæ—¨åœ¨ä¸ºæ‚¨èŠ‚çœæ—¶é—´ â±ï¸ã€‚

ğŸ”¨ åœ¨<b>å®éªŒç®¡ç†å™¨</b>ä¸­è·Ÿè¸ªæ¯ä¸ªYOLOv5è®­ç»ƒè¿è¡Œ

ğŸ”§ ä½¿ç”¨é›†æˆçš„ClearML <b>æ•°æ®ç‰ˆæœ¬æ§åˆ¶å·¥å…·</b>è¿›è¡Œç‰ˆæœ¬æ§åˆ¶å¹¶è½»æ¾è®¿é—®æ‚¨çš„è‡ªå®šä¹‰è®­ç»ƒæ•°æ®

ğŸ”¦ ä½¿ç”¨ClearML Agent <b>è¿œç¨‹è®­ç»ƒå’Œç›‘æ§</b>æ‚¨çš„YOLOv5è®­ç»ƒè¿è¡Œ

ğŸ”¬ ä½¿ç”¨ClearML <b>è¶…å‚æ•°ä¼˜åŒ–</b>è·å¾—æœ€ä½³çš„mAP

ğŸ”­ ä½¿ç”¨ClearML Servingé€šè¿‡å‡ ä¸ªå‘½ä»¤å°†æ–°è®­ç»ƒçš„<b>YOLOv5æ¨¡å‹å˜æˆAPI</b>

<br>
æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨å¤šå°‘è¿™äº›å·¥å…·ï¼Œå¯ä»¥ä»…ä½¿ç”¨å®éªŒç®¡ç†å™¨ï¼Œæˆ–è€…å°†å®ƒä»¬å…¨éƒ¨ç»„åˆæˆä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„ç®¡é“ï¼
<br>
<br>

![ClearML scalars dashboard](https://github.com/thepycoder/clearml_screenshots/raw/main/experiment_manager_with_compare.gif)

<br>
<br>

## ğŸ¦¾ è®¾ç½®

è¦è·Ÿè¸ªæ‚¨çš„å®éªŒå’Œ/æˆ–æ•°æ®ï¼ŒClearMLéœ€è¦ä¸æœåŠ¡å™¨é€šä¿¡ã€‚æ‚¨æœ‰ä¸¤ç§é€‰æ‹©ï¼š

å¯ä»¥å…è´¹æ³¨å†Œ[ClearMLæ‰˜ç®¡æœåŠ¡](https://clear.ml/)ï¼Œæˆ–è€…å¯ä»¥è®¾ç½®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œè¯¦è§[è¿™é‡Œ](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server)ã€‚å³ä½¿æ˜¯æœåŠ¡å™¨ä¹Ÿæ˜¯å¼€æºçš„ï¼Œå› æ­¤å³ä½¿æ‚¨å¤„ç†æ•æ„Ÿæ•°æ®ï¼Œä¹Ÿåº”è¯¥æ²¡é—®é¢˜ï¼

- å®‰è£… `clearml` pythonåŒ…ï¼š

    ```bash
    pip install clearml
    ```

- é€šè¿‡[åˆ›å»ºå‡­è¯](https://app.clear.ml/settings/workspace-configuration)å°†ClearML SDKè¿æ¥åˆ°æœåŠ¡å™¨ï¼ˆè¿›å…¥å³ä¸Šè§’çš„è®¾ç½® -> å·¥ä½œåŒº -> åˆ›å»ºæ–°å‡­è¯ï¼‰ï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¹¶æŒ‰ç…§æŒ‡ç¤ºæ“ä½œï¼š

    ```bash
    clearml-init
    ```

å°±æ˜¯è¿™æ ·ï¼æ‚¨å·²ç»å®Œæˆäº†ğŸ˜

<br>

## ğŸš€ ä½¿ç”¨ClearMLè®­ç»ƒYOLOv5

è¦å¯ç”¨ClearMLå®éªŒè·Ÿè¸ªï¼Œåªéœ€å®‰è£…ClearML pipåŒ…ã€‚

```bash
pip install clearml>=1.2.0

```

This will enable integration with the YOLOv5 training script. Every training run from now on, will be captured and stored by the ClearML experiment manager.
è¿™å°†å¯ç”¨ä¸YOLOv5è®­ç»ƒè„šæœ¬çš„é›†æˆã€‚ä»ç°åœ¨å¼€å§‹ï¼Œæ¯æ¬¡è®­ç»ƒè¿è¡Œéƒ½å°†è¢«ClearMLå®éªŒç®¡ç†å™¨æ•è·å’Œå­˜å‚¨ã€‚


If you want to change the `project_name` or `task_name`, use the `--project` and `--name` arguments of the `train.py` script, by default the project will be called `YOLOv5` and the task `Training`. PLEASE NOTE: ClearML uses `/` as a delimiter for subprojects, so be careful when using `/` in your project name!
å¦‚æœæ‚¨æƒ³æ›´æ”¹project_nameæˆ–task_nameï¼Œè¯·ä½¿ç”¨train.pyè„šæœ¬çš„--projectå’Œ--nameå‚æ•°ï¼Œé»˜è®¤æƒ…å†µä¸‹é¡¹ç›®å°†è¢«ç§°ä¸ºYOLOv5ï¼Œä»»åŠ¡ä¸ºTrainingã€‚è¯·æ³¨æ„ï¼šClearMLä½¿ç”¨/ä½œä¸ºå­é¡¹ç›®çš„åˆ†éš”ç¬¦ï¼Œå› æ­¤åœ¨ä½¿ç”¨/æ—¶è¯·å°å¿ƒï¼


```bash
python train.py --img 640 --batch 16 --epochs 3 --data coco8.yaml --weights yolov5s.pt --cache
```

or with custom project and task name:
æˆ–è€…ä½¿ç”¨è‡ªå®šä¹‰é¡¹ç›®å’Œä»»åŠ¡åç§°ï¼š

```bash
python train.py --project my_project --name my_training --img 640 --batch 16 --epochs 3 --data coco8.yaml --weights yolov5s.pt --cache
```

This will capture:
è¿™å°†æ•è·ï¼š

æºä»£ç å’Œæœªæäº¤çš„æ›´æ”¹
å·²å®‰è£…çš„è½¯ä»¶åŒ…
ï¼ˆè¶…ï¼‰å‚æ•°
æ¨¡å‹æ–‡ä»¶ï¼ˆä½¿ç”¨--save-period næ¯nä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼‰
æ§åˆ¶å°è¾“å‡º
æ ‡é‡ï¼ˆmAP_0.5ã€mAP_0.5:0.95ã€ç²¾åº¦ã€å¬å›ç‡ã€æŸå¤±ã€å­¦ä¹ ç‡ç­‰ï¼‰
æœºå™¨è¯¦ç»†ä¿¡æ¯ã€è¿è¡Œæ—¶é—´ã€åˆ›å»ºæ—¥æœŸç­‰ä¸€èˆ¬ä¿¡æ¯
æ‰€æœ‰ç”Ÿæˆçš„å›¾ï¼Œå¦‚æ ‡ç­¾ç›¸å…³å›¾å’Œæ··æ·†çŸ©é˜µ
æ¯ä¸ªepochçš„è¾¹æ¡†å›¾åƒ
æ¯ä¸ªepochçš„æ‹¼å›¾
æ¯ä¸ªepochçš„éªŒè¯å›¾åƒ
- Source code + uncommitted changes
- Installed packages
- (Hyper)parameters
- Model files (use `--save-period n` to save a checkpoint every n epochs)
- Console output
- Scalars (mAP_0.5, mAP_0.5:0.95, precision, recall, losses, learning rates, ...)
- General info such as machine details, runtime, creation date etc.
- All produced plots such as label correlogram and confusion matrix
- Images with bounding boxes per epoch
- Mosaic per epoch
- Validation images per epoch

That's a lot right? ğŸ¤¯ Now, we can visualize all of this information in the ClearML UI to get an overview of our training progress. Add custom columns to the table view (such as e.g. mAP_0.5) so you can easily sort on the best performing model. Or select multiple experiments and directly compare them!
è¿™å¾ˆå¤šï¼Œå¯¹å§ï¼ŸğŸ¤¯ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ClearML UIä¸­å¯è§†åŒ–æ‰€æœ‰è¿™äº›ä¿¡æ¯ï¼Œä»¥è·å¾—è®­ç»ƒè¿›å±•çš„æ¦‚è§ˆã€‚å°†è‡ªå®šä¹‰åˆ—æ·»åŠ åˆ°è¡¨æ ¼è§†å›¾ï¼ˆä¾‹å¦‚mAP_0.5ï¼‰ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è½»æ¾åœ°å¯¹æœ€ä½³è¡¨ç°çš„æ¨¡å‹è¿›è¡Œæ’åºã€‚æˆ–è€…é€‰æ‹©å¤šä¸ªå®éªŒå¹¶ç›´æ¥æ¯”è¾ƒå®ƒä»¬ï¼


There even more we can do with all of this information, like hyperparameter optimization and remote execution, so keep reading if you want to see how that works!
æˆ‘ä»¬è¿˜å¯ä»¥ç”¨æ‰€æœ‰è¿™äº›ä¿¡æ¯åšæ›´å¤šçš„äº‹æƒ…ï¼Œå¦‚è¶…å‚æ•°ä¼˜åŒ–å’Œè¿œç¨‹æ‰§è¡Œï¼Œæ‰€ä»¥å¦‚æœæ‚¨æƒ³äº†è§£å…¶å·¥ä½œåŸç†ï¼Œè¯·ç»§ç»­é˜…è¯»ï¼


### ğŸ”— Dataset Version ManagementğŸ”— æ•°æ®é›†ç‰ˆæœ¬ç®¡ç†


Versioning your data separately from your code is generally a good idea and makes it easy to acquire the latest version too. This repository supports supplying a dataset version ID, and it will make sure to get the data if it's not there yet. Next to that, this workflow also saves the used dataset ID as part of the task parameters, so you will always know for sure which data was used in which experiment!
å°†æ•°æ®ä¸ä»£ç åˆ†å¼€è¿›è¡Œç‰ˆæœ¬æ§åˆ¶é€šå¸¸æ˜¯ä¸ªå¥½ä¸»æ„ï¼Œå¹¶ä¸”ä½¿å¾—è·å–æœ€æ–°ç‰ˆæœ¬å˜å¾—å®¹æ˜“ã€‚è¯¥ä»“åº“æ”¯æŒæä¾›æ•°æ®é›†ç‰ˆæœ¬IDï¼Œå¹¶ç¡®ä¿åœ¨æ•°æ®ä¸å­˜åœ¨æ—¶è·å–æ•°æ®ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œæµè¿˜å°†ä½¿ç”¨çš„æ•°æ®é›†IDä¿å­˜ä¸ºä»»åŠ¡å‚æ•°çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤æ‚¨å°†å§‹ç»ˆçŸ¥é“åœ¨æ¯ä¸ªå®éªŒä¸­ä½¿ç”¨äº†å“ªäº›æ•°æ®ï¼


![ClearML Dataset Interface](https://github.com/thepycoder/clearml_screenshots/raw/main/clearml_data.gif)

### Prepare Your Datasetå‡†å¤‡æ‚¨çš„æ•°æ®é›†


The YOLOv5 repository supports a number of different datasets by using YAML files containing their information. By default datasets are downloaded to the `../datasets` folder in relation to the repository root folder. So if you downloaded the `coco128` dataset using the link in the YAML or with the scripts provided by yolov5, you get this folder structure:
YOLOv5ä»“åº“æ”¯æŒé€šè¿‡ä½¿ç”¨åŒ…å«å…¶ä¿¡æ¯çš„YAMLæ–‡ä»¶æ¥æ”¯æŒè®¸å¤šä¸åŒçš„æ•°æ®é›†ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ•°æ®é›†å°†ä¸‹è½½åˆ°ä¸ä»“åº“æ ¹æ–‡ä»¶å¤¹ç›¸å…³çš„../datasetsæ–‡ä»¶å¤¹ä¸­ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨ä½¿ç”¨YAMLä¸­çš„é“¾æ¥æˆ–yolov5æä¾›çš„è„šæœ¬ä¸‹è½½äº†coco128æ•°æ®é›†ï¼Œæ‚¨å°†å¾—åˆ°ä»¥ä¸‹æ–‡ä»¶å¤¹ç»“æ„ï¼š


```
..
|_ yolov5
|_ datasets
    |_ coco128
        |_ images
        |_ labels
        |_ LICENSE
        |_ README.txt
```

But this can be any dataset you wish. Feel free to use your own, as long as you keep to this folder structure.
ä½†è¿™å¯ä»¥æ˜¯ä»»ä½•æ‚¨å¸Œæœ›çš„æ•°æ®é›†ã€‚åªè¦ä¿æŒæ­¤æ–‡ä»¶å¤¹ç»“æ„ï¼Œéšæ—¶å¯ä»¥ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†ã€‚


Next, âš ï¸**copy the corresponding YAML file to the root of the dataset folder**âš ï¸. This YAML files contains the information ClearML will need to properly use the dataset. You can make this yourself too, of course, just follow the structure of the example YAMLs.
æ¥ä¸‹æ¥ï¼Œâš ï¸å°†ç›¸åº”çš„YAMLæ–‡ä»¶å¤åˆ¶åˆ°æ•°æ®é›†æ–‡ä»¶å¤¹çš„æ ¹ç›®å½•âš ï¸ã€‚æ­¤YAMLæ–‡ä»¶åŒ…å«ClearMLæ­£ç¡®ä½¿ç”¨æ•°æ®é›†æ‰€éœ€çš„ä¿¡æ¯ã€‚æ‚¨ä¹Ÿå¯ä»¥è‡ªå·±åˆ¶ä½œè¿™ä¸ªæ–‡ä»¶ï¼Œåªéœ€éµå¾ªç¤ºä¾‹YAMLçš„ç»“æ„å³å¯ã€‚


Basically we need the following keys: `path`, `train`, `test`, `val`, `nc`, `names`.
åŸºæœ¬ä¸Šæˆ‘ä»¬éœ€è¦ä»¥ä¸‹é”®ï¼špathã€trainã€testã€valã€ncã€namesã€‚


```
..
|_ yolov5
|_ datasets
    |_ coco128
        |_ images
        |_ labels
        |_ coco128.yaml  # <---- HERE!
        |_ LICENSE
        |_ README.txt
```

### Upload Your Dataset
ä¸Šä¼ æ‚¨çš„æ•°æ®é›†

To get this dataset into ClearML as a versioned dataset, go to the dataset root folder and run the following command:
è¦å°†æ­¤æ•°æ®é›†ä½œä¸ºç‰ˆæœ¬æ§åˆ¶æ•°æ®é›†ä¸Šä¼ åˆ°ClearMLï¼Œè¯·è½¬åˆ°æ•°æ®é›†æ ¹æ–‡ä»¶å¤¹å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š


```bash
cd coco128
clearml-data sync --project YOLOv5 --name coco128 --folder .
```

The command `clearml-data sync` is actually a shorthand command. You could also run these commands one after the other:
å‘½ä»¤clearml-data syncå®é™…ä¸Šæ˜¯ä¸€ä¸ªç®€å†™å‘½ä»¤ã€‚æ‚¨ä¹Ÿå¯ä»¥æŒ‰é¡ºåºè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š


```bash
# å¦‚æœè¦åŸºäºå¦ä¸€ä¸ªæ•°æ®é›†ç‰ˆæœ¬ï¼Œè¯·é€‰æ‹©--parent <parent_dataset_id>
# è¿™æ ·ä¸ä¼šä¸Šä¼ é‡å¤çš„æ–‡ä»¶ï¼
clearml-data create --name coco128 --project YOLOv5
clearml-data add --files .
clearml-data close
```

### Run Training Using A ClearML Datasetä½¿ç”¨ClearMLæ•°æ®é›†è¿è¡Œè®­ç»ƒ

Now that you have a ClearML dataset, you can very simply use it to train custom YOLOv5 ğŸš€ models!
ç°åœ¨æ‚¨æœ‰äº†ä¸€ä¸ªClearMLæ•°æ®é›†ï¼Œæ‚¨å¯ä»¥éå¸¸ç®€å•åœ°ä½¿ç”¨å®ƒæ¥è®­ç»ƒè‡ªå®šä¹‰çš„YOLOv5 ğŸš€æ¨¡å‹ï¼

```bash
python train.py --img 640 --batch 16 --epochs 3 --data clearml://<your_dataset_id> --weights yolov5s.pt --cache
```

<br>

### ğŸ‘€ Hyperparameter Optimization
ğŸ‘€ è¶…å‚æ•°ä¼˜åŒ–

Now that we have our experiments and data versioned, it's time to take a look at what we can build on top!
ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹å®éªŒå’Œæ•°æ®è¿›è¡Œäº†ç‰ˆæœ¬æ§åˆ¶ï¼Œæ˜¯æ—¶å€™çœ‹çœ‹æˆ‘ä»¬å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºä»€ä¹ˆäº†ï¼


Using the code information, installed packages and environment details, the experiment itself is now **completely reproducible**. In fact, ClearML allows you to clone an experiment and even change its parameters. We can then just rerun it with these new parameters automatically, this is basically what HPO does!
ä½¿ç”¨ä»£ç ä¿¡æ¯ã€å·²å®‰è£…çš„è½¯ä»¶åŒ…å’Œç¯å¢ƒè¯¦ç»†ä¿¡æ¯ï¼Œå®éªŒæœ¬èº«ç°åœ¨æ˜¯å®Œå…¨å¯é‡ç°çš„ã€‚å®é™…ä¸Šï¼ŒClearMLå…è®¸æ‚¨å…‹éš†ä¸€ä¸ªå®éªŒå¹¶æ›´æ”¹å…¶å‚æ•°ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è‡ªåŠ¨è¿è¡Œå®ƒï¼Œè¿™åŸºæœ¬ä¸Šå°±æ˜¯HPOï¼ˆè¶…å‚æ•°ä¼˜åŒ–ï¼‰æ‰€åšçš„ï¼


To **run hyperparameter optimization locally**, we've included a pre-made script for you. Just make sure a training task has been run at least once, so it is in the ClearML experiment manager, we will essentially clone it and change its hyperparameters.
è¦æœ¬åœ°è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œæˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªé¢„åˆ¶è„šæœ¬ã€‚åªéœ€ç¡®ä¿è‡³å°‘è¿è¡Œè¿‡ä¸€æ¬¡è®­ç»ƒä»»åŠ¡ï¼Œä»¥ä¾¿å®ƒåœ¨ClearMLå®éªŒç®¡ç†å™¨ä¸­ï¼Œæˆ‘ä»¬å°†å…‹éš†å®ƒå¹¶æ›´æ”¹å…¶è¶…å‚æ•°ã€‚


You'll need to fill in the ID of this `template task` in the script found at `utils/loggers/clearml/hpo.py` and then just run it :) You can change `task.execute_locally()` to `task.execute()` to put it in a ClearML queue and have a remote agent work on it instead.
æ‚¨éœ€è¦åœ¨utils/loggers/clearml/hpo.pyè„šæœ¬ä¸­å¡«å†™æ­¤æ¨¡æ¿ä»»åŠ¡çš„IDï¼Œç„¶åè¿è¡Œå®ƒ :) æ‚¨å¯ä»¥å°†task.execute_locally()æ›´æ”¹ä¸ºtask.execute()ï¼Œä»¥å°†å…¶æ”¾å…¥ClearMLé˜Ÿåˆ—ï¼Œå¹¶ç”±è¿œç¨‹ä»£ç†å¤„ç†ã€‚


```bash
# è¦ä½¿ç”¨optunaï¼Œè¯·å…ˆå®‰è£…å®ƒï¼Œå¦åˆ™å¯ä»¥å°†ä¼˜åŒ–å™¨æ›´æ”¹ä¸ºRandomSearch
pip install optuna
python utils/loggers/clearml/hpo.py

```

![HPO](https://github.com/thepycoder/clearml_screenshots/raw/main/hpo.png)

## ğŸ¤¯ Remote Execution (advanced) ğŸ¤¯ è¿œç¨‹æ‰§è¡Œï¼ˆé«˜çº§ï¼‰


Running HPO locally is really handy, but what if we want to run our experiments on a remote machine instead? Maybe you have access to a very powerful GPU machine on-site, or you have some budget to use cloud GPUs. This is where the ClearML Agent comes into play. Check out what the agent can do here:
æœ¬åœ°è¿è¡ŒHPOéå¸¸æ–¹ä¾¿ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³åœ¨è¿œç¨‹æœºå™¨ä¸Šè¿è¡Œå®éªŒæ€ä¹ˆåŠï¼Ÿä¹Ÿè®¸æ‚¨æœ‰è®¿é—®ç°åœºéå¸¸å¼ºå¤§çš„GPUæœºå™¨çš„æƒé™ï¼Œæˆ–è€…æ‚¨æœ‰ä¸€äº›é¢„ç®—ç”¨äºäº‘GPUã€‚è¿™å°±æ˜¯ClearML Agentçš„ç”¨æ­¦ä¹‹åœ°ã€‚çœ‹çœ‹ä»£ç†å¯ä»¥åšäº›ä»€ä¹ˆï¼š


- [YouTube video](https://youtu.be/MX3BrXnaULs)
- [Documentation](https://clear.ml/docs/latest/docs/clearml_agent)

In short: every experiment tracked by the experiment manager contains enough information to reproduce it on a different machine (installed packages, uncommitted changes etc.). So a ClearML agent does just that: it listens to a queue for incoming tasks and when it finds one, it recreates the environment and runs it while still reporting scalars, plots etc. to the experiment manager.
ç®€è€Œè¨€ä¹‹ï¼šå®éªŒç®¡ç†å™¨è·Ÿè¸ªçš„æ¯ä¸ªå®éªŒéƒ½åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„æœºå™¨ä¸Šé‡ç°å®ƒï¼ˆå·²å®‰è£…çš„è½¯ä»¶åŒ…ã€æœªæäº¤çš„æ›´æ”¹ç­‰ï¼‰ã€‚å› æ­¤ï¼ŒClearMLä»£ç†å°±æ˜¯è¿™æ ·åšçš„ï¼šå®ƒç›‘å¬é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼Œä¸€æ—¦æ‰¾åˆ°ä»»åŠ¡ï¼Œå®ƒå°±ä¼šé‡æ–°åˆ›å»ºç¯å¢ƒå¹¶è¿è¡Œå®ƒï¼ŒåŒæ—¶ä»ç„¶å‘å®éªŒç®¡ç†å™¨æŠ¥å‘Šæ ‡é‡ã€å›¾ç­‰ã€‚


You can turn any machine (a cloud VM, a local GPU machine, your own laptop ... ) into a ClearML agent by simply running:
æ‚¨å¯ä»¥é€šè¿‡ç®€å•è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†ä»»ä½•æœºå™¨ï¼ˆäº‘VMã€æœ¬åœ°GPUæœºå™¨ã€æ‚¨çš„ç¬”è®°æœ¬ç”µè„‘â€¦â€¦ï¼‰å˜æˆClearMLä»£ç†ï¼š


```bash
clearml-agent daemon --queue <queues_to_listen_to> [--docker]
```

### Cloning, Editing And Enqueuing
å…‹éš†ã€ç¼–è¾‘å’Œå…¥é˜Ÿ

With our agent running, we can give it some work. Remember from the HPO section that we can clone a task and edit the hyperparameters? We can do that from the interface too!
ä»£ç†è¿è¡Œåï¼Œæˆ‘ä»¬å¯ä»¥ç»™å®ƒä¸€äº›å·¥ä½œã€‚è®°å¾—åœ¨HPOéƒ¨åˆ†æˆ‘ä»¬å¯ä»¥å…‹éš†ä»»åŠ¡å¹¶ç¼–è¾‘è¶…å‚æ•°å—ï¼Ÿæˆ‘ä»¬ä¹Ÿå¯ä»¥ä»ç•Œé¢ä¸­åšåˆ°è¿™ä¸€ç‚¹ï¼


ğŸª„ Clone the experiment by right-clicking it
ğŸª„ å³é”®å•å‡»å®éªŒä»¥å…‹éš†å®ƒ


ğŸ¯ Edit the hyperparameters to what you wish them to be
ğŸ¯ ç¼–è¾‘æ‚¨å¸Œæœ›çš„è¶…å‚æ•°


â³ Enqueue the task to any of the queues by right-clicking it
â³ å³é”®å•å‡»ä»»åŠ¡å°†å…¶å…¥é˜Ÿåˆ°ä»»æ„é˜Ÿåˆ—


![Enqueue a task from the UI](https://github.com/thepycoder/clearml_screenshots/raw/main/enqueue.gif)

### Executing A Task Remotelyè¿œç¨‹æ‰§è¡Œä»»åŠ¡


Now you can clone a task like we explained above, or simply mark your current script by adding `task.execute_remotely()` and on execution it will be put into a queue, for the agent to start working on!
ç°åœ¨æ‚¨å¯ä»¥æŒ‰ç…§ä¸Šè¿°è¯´æ˜å…‹éš†ä»»åŠ¡ï¼Œæˆ–è€…åªéœ€é€šè¿‡æ·»åŠ task.execute_remotely()æ ‡è®°å½“å‰è„šæœ¬ï¼Œå¹¶åœ¨æ‰§è¡Œæ—¶å°†å…¶æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±ä»£ç†å¼€å§‹å·¥ä½œï¼


To run the YOLOv5 training script remotely, all you have to do is add this line to the training.py script after the clearml logger has been instantiated:
è¦è¿œç¨‹è¿è¡ŒYOLOv5è®­ç»ƒè„šæœ¬ï¼Œåªéœ€åœ¨å®ä¾‹åŒ–ClearML loggeråå°†æ­¤è¡Œæ·»åŠ åˆ°training.pyè„šæœ¬ä¸­ï¼š


```python
# ...
# æ—¥å¿—è®°å½•å™¨
data_dict = None
if RANK in {-1, 0}:
    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # æ—¥å¿—è®°å½•å™¨å®ä¾‹
    if loggers.clearml:
        loggers.clearml.task.execute_remotely(queue="my_queue")  # <------ æ·»åŠ æ­¤è¡Œ
        # data_dictè¦ä¹ˆä¸ºç©ºï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰é€‰æ‹©ClearMLæ•°æ®é›†ï¼Œè¦ä¹ˆç”±ClearMLå¡«å……
        data_dict = loggers.clearml.data_dict
# ...

```

When running the training script after this change, python will run the script up until that line, after which it will package the code and send it to the queue instead!
åœ¨è¿›è¡Œæ­¤æ›´æ”¹åè¿è¡Œè®­ç»ƒè„šæœ¬æ—¶ï¼Œpythonå°†è¿è¡Œè„šæœ¬ç›´åˆ°è¯¥è¡Œï¼Œç„¶åå°†æ‰“åŒ…ä»£ç å¹¶å‘é€åˆ°é˜Ÿåˆ—ä¸­ï¼


### Autoscaling workersè‡ªåŠ¨æ‰©å±•å·¥ä½œè€…


ClearML comes with autoscalers too! This tool will automatically spin up new remote machines in the cloud of your choice (AWS, GCP, Azure) and turn them into ClearML agents for you whenever there are experiments detected in the queue. Once the tasks are processed, the autoscaler will automatically shut down the remote machines, and you stop paying!
ClearMLè¿˜å¸¦æœ‰è‡ªåŠ¨æ‰©å±•å™¨ï¼è¯¥å·¥å…·å°†åœ¨æ‚¨é€‰æ‹©çš„äº‘ï¼ˆAWSã€GCPã€Azureï¼‰ä¸­è‡ªåŠ¨å¯åŠ¨æ–°çš„è¿œç¨‹æœºå™¨ï¼Œå¹¶å°†å®ƒä»¬å˜æˆClearMLä»£ç†ï¼Œåªè¦æ£€æµ‹åˆ°é˜Ÿåˆ—ä¸­æœ‰å®éªŒã€‚ä¸€æ—¦ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼Œè‡ªåŠ¨æ‰©å±•å™¨å°†è‡ªåŠ¨å…³é—­è¿œç¨‹æœºå™¨ï¼Œæ‚¨ä¹Ÿåœæ­¢ä»˜è´¹ï¼


Check out the autoscalers getting started video below.
è¯·è§‚çœ‹ä¸‹é¢çš„è‡ªåŠ¨æ‰©å±•å™¨å…¥é—¨è§†é¢‘ã€‚


[![Watch the video](https://img.youtube.com/vi/j4XVMAaUt3E/0.jpg)](https://youtu.be/j4XVMAaUt3E)
